{   
    "about":{
        "name":"Rogers Lab",
        "photo":"assets/profile.png",
        "desc":"Sharing research and development projects, tools, and technologies being created in Rogers Lab at Tufts University"
    },
    "footer":{
        "logo":"",
        "ceeologo":"assets/CEEO-logo.png",
        "about":"Rogers Lab, Tufts University",
        "links":""
    },
    "news":[
        {
            "title":"Launching XXX",
            "date": "Mar 3, 2021"
        },
        {
            "title":"Presenting YYY",
            "date": "Jan 13, 2021"
        },
        {
            "title":"Traveling to Korea",
            "date": "Dec 12, 2020"
        },
        {
            "title":"Serving as a program committee for IEEE",
            "date": "Sep 18, 2020"
        },
        {
            "title":"Demonstrating ZZZ",
            "date": "Sep 11, 2020"
        },
        {
            "title":"Attending HRI 2020",
            "date": "May 04, 2020 ~ May 09, 2020"
        }
    ],
    "projects":[
        {
            "id":"roboticsplayground",
            "title": "Robotics Playground",
            "subtitle": "Experimental library of placemat instructions for open-ended robotics challenges",
            "authors": "Sara Willner-Giwerc, Chris Rogers",
            "teaser": "assets/roboticsplayground/teaser.png",
            "tags":["Robotics", "Educational Tool"],
            "materials": [
                { 
                    "label": "Paper",
                    "path": "assets/roboticsplayground/paper.pdf"
                },
                { 
                    "label": "Video",
                    "path": "assets/roboticsplayground/video.mp4"
                },
                { 
                    "label": "Demo",
                    "path": "https://ceeoinnovations.github.io/RoboticsPlayground/index.html"
                }
            ],
            "desc": "Robotics Playground is an experimental library of placemat instructions for robotics activities. Developed by researchers at the Tufts University CEEO, this library is designed to maximize solution diversity and foster student-driven learning."
        },
        {
            "id":"snapr",
            "title": "SNAPR: Snap, Narrate, Auto post and Refelect",
            "subtitle": "Automatic e-portfolio generator",
            "authors": "Dipeshwor Man Shrestha, Chris Rogers",
            "teaser": "assets/snapr/teaser.png",
            "tags":["Engineering", "System"],
            "materials": [
                { 
                    "label": "Paper",
                    "path": "assets/snapr/paper.pdf"
                },
                { 
                    "label": "Video",
                    "path": ""
                },
                { 
                    "label": "Demo",
                    "path": "https://github.com/dipeshwor/SNAPR"
                }
            ],
            "desc": "In this paper, we present BubbleView, an alternative methodology for eye tracking using discrete mouse clicks to measure which information people consciously choose to examine. BubbleView is a mouse-contingent, moving-window interface in which participants are presented with a series of blurred images and click to reveal 'bubbles' - small, circular areas of the image at original resolution, similar to having a confined area of focus like the eye fovea. Across 10 experiments with 28 different parameter combinations, we evaluated BubbleView on a variety of image types: information visualizations, natural images, static webpages, and graphic designs, and compared the clicks to eye fixations collected with eye-trackers in controlled lab settings. We found that BubbleView clicks can both (i) successfully approximate eye fixations on different images, and (ii) be used to rank image and design elements by importance. BubbleView is designed to collect clicks on static images, and works best for defined tasks such as describing the content of an information visualization or measuring image importance. BubbleView data is cleaner and more consistent than related methodologies that use continuous mouse movements. Our analyses validate the use of mouse-contingent, moving-window methodologies as approximating eye fixations for different image and task types."
        },
        {
            "id":"draw2code",
            "title": "Draw2Code",
            "subtitle": "Low-cost and expressive tangible computational kit",
            "authors": "Hyejin Im, Chris Rogers",
            "teaser": "assets/draw2code/teaser.png",
            "tags":["Computational Thinking", "System"],
            "materials": [
                { 
                    "label": "Paper",
                    "path": ""
                },
                { 
                    "label": "Video",
                    "path": ""
                },
                { 
                    "label": "Demo",
                    "path": "https://hyejinim.github.io/draw2code"
                }
            ],
            "desc": "Comics are an entertaining and familiar medium for presenting compelling stories about data. However, existing visualization authoring tools do not leverage this expressive medium. In this paper, we seek to incorporate elements of comics into the construction of data-driven stories about dynamic networks. We contribute DataToon, a flexible data comic storyboarding tool that blends analysis and presentation with pen and touch interactions. A storyteller can use DataToon rapidly generate visualization panels, annotate them, and position them within a canvas to produce a visually compelling narrative. In a user study, participants quickly learned to use DataToon for producing data comics."
        }

    ]
}